package mainclass;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;

public class InvertIndex {

	public static class InvertIndexMapper extends Mapper<Object, Text, Text, IntWritable> {
		
		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
			FileSplit filesplit = (FileSplit) context.getInputSplit();
			String filename = filesplit.getPath().getName();
			String temp = new String();
			String line = value.toString();
			StringTokenizer itr = new StringTokenizer(line, " ");
			while(itr.hasMoreTokens()){
				temp = itr.nextToken();		
				//we can add stopwords here
				Text word = new Text(temp+"#"+filename);
				context.write(word, new IntWritable(1));
			}
		}

	}
	
	public static class SumCombiner extends Reducer<Text,IntWritable,Text,IntWritable>{
		private IntWritable result = new IntWritable();
		
		public void reduce(Text key,Iterable<IntWritable> values,Context context) throws IOException, InterruptedException{
			int sum = 0;
			for(IntWritable val : values){
				sum += val.get();
			}
			result.set(sum);
			context.write(key, result);
		}
		
	}
	
	public static class SplitPartitioner extends HashPartitioner<Text,IntWritable>{
		
		public int getPartition(Text key,IntWritable value,int numReduceTasks){
			String term = key.toString().split("#")[0];
			return super.getPartition(new Text(term), value, numReduceTasks);
		}
	}
	
	public static class InvertedIndexReducer extends
    Reducer<Text, IntWritable, Text, Text> {
  private Text word1 = new Text();
  private Text word2 = new Text();
  String temp = new String();
  static Text CurrentItem = new Text(" ");
  static List<String> postingList = new ArrayList<String>();

  public void reduce(Text key, Iterable<IntWritable> values, Context context)
      throws IOException, InterruptedException {
    int sum = 0;
    word1.set(key.toString().split("#")[0]);
    temp = key.toString().split("#")[1];
    for (IntWritable val : values) {
      sum += val.get();
    }
    word2.set("<" + temp + "," + sum + ">");
    if (!CurrentItem.equals(word1) && !CurrentItem.equals(" ")) {
      StringBuilder out = new StringBuilder();
      long count = 0;
      for (String p : postingList) {
        out.append(p);
        out.append(";");
        count =
            count
                + Long.parseLong(p.substring(p.indexOf(",") + 1,
                    p.indexOf(">")));
      }
      out.append("<total," + count + ">.");
      if (count > 0)
        context.write(CurrentItem, new Text(out.toString()));
      postingList = new ArrayList<String>();
    }
    CurrentItem = new Text(word1);
    postingList.add(word2.toString()); // 不断向postingList也就是文档名称中添加词表
  }

  // cleanup 一般情况默认为空，有了cleanup不会遗漏最后一个单词的情况

  public void cleanup(Context context) throws IOException,
      InterruptedException {
    StringBuilder out = new StringBuilder();
    long count = 0;
    for (String p : postingList) {
      out.append(p);
      out.append(";");
      count =
          count
              + Long
                  .parseLong(p.substring(p.indexOf(",") + 1, p.indexOf(">")));
    }
    out.append("<total," + count + ">.");
    if (count > 0)
      context.write(CurrentItem, new Text(out.toString()));
  }
}
